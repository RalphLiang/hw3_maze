# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd

class hw3_maze(object):
    def __init__(self):
        self.N_STATES_x=40                        # 2維世界的寬度
        self.N_STATES_y=30                        # 2維世界的深度
        self.ACTIONS=['left','right','up','down'] # 探索者的可用動作
        self.GOAL=1199
        self.EPSILON=0.9                          # 貪婪度 greedy
        self.ALPHA=0.1                            # 學習率
        self.GAMMA=0.9                            # 獎勵遞減值
        self.MAX_EPISODES=50                      # 最大回合數
        self.FRESH_TIME=0                         # 移動間隔時間
        self.REWARD_LIST = [149, 319, 560, 1080, 1173]
        self.WALL_LIST = [1, 4, 12, 14, 16, 18, 20, 22, 25, 27, 31, 33, 34, 36, 41, 43, 46, 49, 51, 52, 53, 57, 61, 63, 65, 66, 68, 70, 72, 75, 78, 85, 88, 90, 100, 102, 104, 107, 109, 113, 116, 117, 122, 125, 127, 131, 134, 135, 138, 141, 150, 152, 155, 158, 161, 162, 164, 166, 170, 172, 175, 176, 177, 180, 182, 184, 186, 187, 189, 191, 196, 197, 199, 201, 203, 205, 206, 207, 209, 210, 212, 214, 218, 221, 224, 228, 232, 233, 238, 245, 248, 251, 255, 257, 258, 260, 262, 263, 266, 269, 271, 274, 279, 281, 286, 287, 289, 294, 296, 301, 304, 306, 308, 310, 315, 316, 318, 321, 323, 325, 328, 331, 332, 333, 335, 337, 338, 340, 342, 344, 347, 351, 353, 355, 357, 360, 362, 366, 367, 369, 371, 378, 381, 383, 388, 389, 391, 394, 398, 403, 405, 406, 408, 411, 413, 414, 415, 416, 418, 420, 422, 424, 426, 428, 430, 433, 435, 440, 441, 451, 453, 455, 458, 460, 467, 469, 471, 472, 477, 479, 483, 486, 489, 491, 494, 496, 498, 502, 504, 505, 507, 508, 509, 518, 522, 524, 527, 529, 530, 531, 533, 538, 539, 541, 543, 548, 551, 552, 554, 557, 559, 561, 563, 566, 568, 573, 575, 577, 579, 582, 584, 585, 587, 589, 592, 598, 600, 604, 605, 609, 610, 611, 613, 616, 618, 621, 623, 628, 631, 632, 634, 636, 639, 641, 643, 646, 648, 651, 653, 655, 657, 659, 662, 664, 665, 666, 667, 669, 673, 677, 680, 682, 685, 687, 689, 693, 701, 703, 711, 712, 714, 717, 719, 723, 728, 731, 733, 735, 737, 738, 740, 742, 744, 746, 747, 749, 751, 752, 754, 756, 758, 760, 761, 763, 766, 771, 774, 776, 779, 781, 783, 788, 794, 796, 799, 800, 802, 809, 810, 814, 816, 822, 824, 826, 829, 831, 833, 838, 844, 845, 847, 848, 851, 853, 857, 859, 866, 868, 870, 872, 874, 877, 878, 879, 880, 881, 883, 885, 886, 889, 894, 895, 896, 898, 901, 904, 905, 907, 911, 914, 918, 927, 931, 935, 937, 938, 941, 943, 944, 946, 947, 956, 957, 960, 962, 963, 964, 966, 969, 970, 974, 977, 979, 982, 984, 985, 990, 992, 994, 996, 998, 1007, 1008, 1011, 1015, 1016, 1018, 1022, 1024, 1026, 1027, 1029, 1031, 1033, 1035, 1039, 1040, 1041, 1044, 1046, 1049, 1052, 1054, 1057, 1059, 1061, 1064, 1065, 1078, 1081, 1083, 1088, 1090, 1091, 1101, 1103, 1104, 1105, 1106, 1107, 1109, 1113, 1116, 1119, 1124, 1125, 1132, 1133, 1136, 1137, 1140, 1141, 1144, 1145, 1148, 1149, 1152, 1153, 1156, 1158, 1161, 1162, 1168, 1169, 1174, 1175, 1178, 1179, 1182, 1183, 1186, 1187, 1190, 1191, 1194, 1195]
        self.env_inuse = []
        self.REWARD_INUSE =[]
        self.SCORE = 0
        self.env_map=[['-','x','-','-','x','-','-','-','-','-','-','-','x','-','x','-','x','-','x','-','x','-','x','-','-','x','-','x','-','-','-','x','-','x','x','-','x','-','-','-'],
                    ['-','x','-','x','-','-','x','-','-','x','-','x','x','x','-','-','-','x','-','-','-','x','-','x','-','x','x','-','x','-','x','-','x','-','-','x','-','-','x','-'],
                    ['-','-','-','-','-','x','-','-','x','-','x','-','-','-','-','-','-','-','-','-','x','-','x','-','x','-','-','x','-','x','-','-','-','x','-','-','x','x','-','-'],
                    ['-','-','x','-','-','x','-','x','-','-','-','x','-','-','x','x','-','-','x','-','-','x','-','-','-','-','-','-','-','o','x','-','x','-','-','x','-','-','x','-'],
                    ['-','x','x','-','x','-','x','-','-','-','x','-','x','-','-','x','x','x','-','-','x','-','x','-','x','-','x','x','-','x','-','x','-','-','-','-','x','x','-','x'],
                    ['-','x','-','x','-','x','x','x','-','x','x','-','x','-','x','-','-','-','x','-','-','x','-','-','x','-','-','-','x','-','-','-','x','x','-','-','-','-','x','-'],
                    ['-','-','-','-','-','x','-','-','x','-','-','x','-','-','-','x','-','x','x','-','x','-','x','x','-','-','x','-','-','x','-','x','-','-','x','-','-','-','-','x'],
                    ['-','x','-','-','-','-','x','x','-','x','-','-','-','-','x','-','x','-','-','-','-','x','-','-','x','-','x','-','x','-','x','-','-','-','-','x','x','-','x','o'],
                    ['-','x','-','x','-','x','-','-','x','-','-','x','x','x','-','x','-','x','x','-','x','-','x','-','x','-','-','x','-','-','-','x','-','x','-','x','-','x','-','-'],
                    ['x','-','x','-','-','-','x','x','-','x','-','x','-','-','-','-','-','-','x','-','-','x','-','x','-','-','-','-','x','x','-','x','-','-','x','-','-','-','x','-'],
                    ['-','-','-','x','-','x','x','-','x','-','-','x','-','x','x','x','x','-','x','-','x','-','x','-','x','-','x','-','x','-','x','-','-','x','-','x','-','-','-','-'],
                    ['x','x','-','-','-','-','-','-','-','-','-','x','-','x','-','x','-','-','x','-','x','-','-','-','-','-','-','x','-','x','-','x','x','-','-','-','-','x','-','x'],
                    ['-','-','-','x','-','-','x','-','-','x','-','x','-','-','x','-','x','-','x','-','-','-','x','-','x','x','-','x','x','x','-','-','-','-','-','-','-','-','x','-'],
                    ['-','-','x','-','x','-','-','x','-','x','x','x','-','x','-','-','-','-','x','x','-','x','-','x','-','-','-','-','x','-','-','x','x','-','x','-','-','x','-','x'],
                    ['o','x','-','x','-','-','x','-','x','-','-','-','-','x','-','x','-','x','-','x','-','-','x','-','x','x','-','x','-','x','-','-','x','-','-','-','-','-','x','-'],
                    ['x','-','-','-','x','x','-','-','-','x','x','x','-','x','-','-','x','-','x','-','-','x','-','x','-','-','-','-','x','-','-','x','x','-','x','-','x','-','-','x'],
                    ['-','x','-','x','-','-','x','-','x','-','-','x','-','x','-','x','-','x','-','x','-','-','x','-','x','x','x','x','-','x','-','-','-','x','-','-','-','x','-','-'],
                    ['x','-','x','-','-','x','-','x','-','x','-','-','-','x','-','-','-','-','-','-','-','x','-','x','-','-','-','-','-','-','-','x','x','-','x','-','-','x','-','x'],
                    ['-','-','-','x','-','-','-','-','x','-','-','x','-','x','-','x','-','x','x','-','x','-','x','-','x','-','x','x','-','x','-','x','x','-','x','-','x','-','x','-'],
                    ['x','x','-','x','-','-','x','-','-','-','-','x','-','-','x','-','x','-','-','x','-','x','-','x','-','-','-','-','x','-','-','-','-','-','x','-','x','-','-','x'],
                    ['x','-','x','-','-','-','-','-','-','x','x','-','-','-','x','-','x','-','-','-','-','-','x','-','x','-','x','-','-','x','-','x','-','x','-','-','-','-','x','-'],
                    ['-','-','-','-','x','x','-','x','x','-','-','x','-','x','-','-','-','x','-','x','-','-','-','-','-','-','x','-','x','-','x','-','x','-','x','-','-','x','x','x'],
                    ['x','x','-','x','-','x','x','-','-','x','-','-','-','-','x','x','x','-','x','-','-','x','-','-','x','x','-','x','-','-','-','x','-','-','x','-','-','-','x','-'],
                    ['-','-','-','-','-','-','-','x','-','-','-','x','-','-','-','x','-','x','x','-','-','x','-','x','x','-','x','x','-','-','-','-','-','-','-','-','x','x','-','-'],
                    ['x','-','x','x','x','-','x','-','-','x','x','-','-','-','x','-','-','x','-','x','-','-','x','-','x','x','-','-','-','-','x','-','x','-','x','-','x','-','x','-'],
                    ['-','-','-','-','-','-','-','x','x','-','-','x','-','-','-','x','x','-','x','-','-','-','x','-','x','-','x','x','-','x','-','x','-','x','-','x','-','-','-','x'],
                    ['x','x','-','-','x','-','x','-','-','x','-','-','x','-','x','-','-','x','-','x','-','x','-','-','x','x','-','-','-','-','-','-','-','-','-','-','-','-','x','-'],
                    ['o','x','-','x','-','-','-','-','x','-','x','x','-','-','-','-','-','-','-','-','-','x','-','x','x','x','x','x','-','x','-','-','-','x','-','-','x','-','-','x'],
                    ['-','-','-','-','x','x','-','-','-','-','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','-','x','-'],
                    ['-','x','x','-','-','-','-','-','x','x','-','-','-','o','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','x','x','-','-','-','T']]

    def build_q_table(self):
        table = pd.DataFrame(
            np.zeros((self.N_STATES_x*self.N_STATES_y,len(self.ACTIONS))),    # q_table 全 0 初始
            columns=self.ACTIONS,)                                            # columns 對應的是行為名稱
        return table

    def choose_action(self,state,q_table):
        state_actions=q_table.iloc[state,:]                                   # 選出這個 state 的所有 action 值
        if (np.random.uniform()>self.EPSILON) or ((state_actions==0).all()):  # 非貪婪 or 或者這個 state 還沒有探索過
            action_name=np.random.choice(self.ACTIONS)
        else:
            action_name=state_actions.idxmax()                                # 貪婪模式
        return action_name

    def get_env_feedback(self,S,A,path):
        # This is how agent will interact with the environment
        if A=='left':             # move left

            if S % self.N_STATES_x ==0:
                S_ = S
                R = -5
            else:
                S_ = S - 1     #左移
                if S_ in self.WALL_LIST:
                  S_ = S
                  R = -5
                elif S_ in self.REWARD_INUSE:
                  self.REWARD_INUSE.remove(S_)
                  self.SCORE += 1
                  R = 7
                elif S_ in path:
                  R = -3
                else:
                  R = 3
        elif A=='right':           # move right
            if S == self.GOAL - 1:     # terminate
                S_ = "terminal"
                R = 10
            elif S % self.N_STATES_x == self.N_STATES_x - 1:
                S_ = S
                R = -5
            else:
                S_ = S + 1     #右移
                if S_ in self.WALL_LIST:
                  S_ = S
                  R = -5
                elif S_ in self.REWARD_INUSE:
                  self.REWARD_INUSE.remove(S_)
                  self.SCORE += 1
                  R = 7
                elif S_ in path:
                  R = -3
                else:
                  R = 3
        elif  A=='up':            # move up
            if int(S/self.N_STATES_x) == 0:
                S_ = S
                R = -5
            else:
                S_ = S - self.N_STATES_x
                if S_ in self.WALL_LIST:
                  S_ = S
                  R = -5
                elif S_ in self.REWARD_INUSE:
                  self.REWARD_INUSE.remove(S_)
                  self.SCORE += 1
                  R = 7
                elif S_ in path:
                  R = -3
                else:
                  R = 3
        else:                  # move down
            if S == self.GOAL - 1:
                S_ = "terminal"    # terminate
                R = 10
            elif int(S/self.N_STATES_x) == self.N_STATES_y - 1:
                S_ = S
                R = -5
            else:
                S_ = S + self.N_STATES_x
                if S_ in self.WALL_LIST:
                  S_ = S
                  R = -5
                elif S_ in self.REWARD_INUSE:
                  self.REWARD_INUSE.remove(S_)
                  self.SCORE += 1
                  R = 7
                elif S_ in path:
                  R = -3
                else:
                  R = 3
        return S_,R

    def update_env(self,S,episode,step_counter):

        result = []
        if S=='terminal':
            interaction='Episode %s: total_steps=%s' % (episode+1,step_counter)
            result.append(interaction)
            print('Score: ', self.SCORE, 'Result: ', result)
            #for y in range(30):
            #    print(self.env_inuse[y])
            print('\r' ,end='')
        else:
            self.env_inuse[int(S/self.N_STATES_x)][int(S%self.N_STATES_x)] = 'm'
            #time.sleep(FRESH_TIME)

    def rl(self):
        q_table=self.build_q_table()                                    # 初始 q table
        for episode in range(self.MAX_EPISODES):                        # 回合
            step_counter = 0
            S = 0                                                       # 回合初始位置
            is_terminated = False                                       # 是否回合結束
            path = []
            self.SCORE = 0
            self.env_inuse = self.env_map
            self.REWARD_INUSE = self.REWARD_LIST
            self.update_env(S,episode,step_counter)                     # 環境更新
            while not is_terminated:
                A = self.choose_action(S,q_table)                       # 選行為
                path.append(S)
                S_, R = self.get_env_feedback(S,A,path)                 # 實施行為並得到環境的反饋
                q_predict = q_table.loc[S,A]                            # 估算的(狀態-行為)值
                if S_ != 'terminal':
                    q_target = R+self.GAMMA*q_table.iloc[S_,:].max()    # 實際的(狀態-行為)值 (回合沒結束)
                else:
                    q_target = R                                        # 實際的(狀態-行為)值 (回合結束)
                    is_terminated=True                                  # terminate this episode

                q_table.loc[S,A] += self.ALPHA*(q_target-q_predict)     #  q_table 更新
                S = S_                                                  # 探索者移動到下一個 state

                self.update_env(S,episode,step_counter+1)               # 環境更新
                step_counter += 1
        return all,q_table

if __name__=="__main__":
    runQL = hw3_maze()
    q_table = runQL.rl()
    print('\r\nQ-table:\n')
    print(q_table)